---
title: "class 08: Brest Cancer Mini Project"
Authour: "Melanda Aboueid (PID A17473102)"
format: pdf
---

## Background 

In today's class we will apply the methods and techniques clustering and PCA to help make sense of a real world breast cancer FNA biopsy data set.

## Data Import 

We start by importing our data. It is a CSV file so we will use the `read.csv()` function. 

```{r}
wisc.df <- read.csv("wisconsinCancer.csv", row.names = 1)
```

have a wee peak at the first few rows

```{r}
head(wisc.df)
```

Make sure to remove the first `diagnosis` colum - I dont want to use this for my mashine learning models. We weill use it later to compare our results to the expert diagnosis.

```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- wisc.df$diagnosis
```

## Exploratory data analysis

> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

> Q2. How many of the observations have a malignant diagnosis?

```{r}
sum( wisc.df$diagnosis == "M")
```
```{r}
table(wisc.df$diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
#colnames(wisc.data)
length(grep("_mean", colnames(wisc.data)))
```

##  Principal Component Analysis 

The main data function here is `prcomp()` and we want to make sure we set the optional argument `scale=TRUE`:
```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal component (PC1)?
0.4427 or 44%

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
at least 3 PC's would be 72%

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
at least 7 PC's would yeild 91%


Our main PCP "score plot" of results: 
```{r}
library(ggplot2)
```
```{r}
ggplot(wisc.pr$x) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
I notice that the similar compnets cluster together either B or M but it is dificult to understand because we need to make our own plots so we can make sense of these PCA's. The low weight PC dont contrubute as much.

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
I notice they are more toweds the bottom of the graph now 

```{r}
ggplot(wisc.pr$x) + 
  aes(PC1, PC3, col=diagnosis) + 
  geom_point()
```




Scree-plot:
```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

```{r}
ggplot(wisc.pr$rotation) + 
  aes(PC1, 
      order( rownames(wisc.pr$rotation), PC1) ) +
  geom_col()
```



Collectively these two plots ("score plot" and "loading plot") tell us that if cells nucleus are deeply indented ("concave"), irregular non circular ("compactness"), and have large "perimeter" values they tend to be malignant.... 

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC. Are there any features with larger contributions than this one?

```{r}
wisc.pr$rotation["concave.points_mean", 1]
```

## Hierarchical clustering

First scale the data (with the `scale()` function), then calculate a distance matrix (with the `dist()` function). Then cluster with the `hclust()` function and plot: 

```{r}
wisc.hclust <- hclust (dist( scale(wisc.data) ) )
```

> Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=20, col="red")
```



you can also use `cutree()` function with argument `k=4` rather than`h=hight`
```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters)
```


##  Combining methods

Here we will taek out PCA results and use thoes as input for clustering. i other words our `wisc.pr$x` scores that we plotted above (the main output from PCA - how the data lie on our new principal componet axis/variables) and use the subset of these PC's that capture the most variance as input for `hclust()`.

```{r}
pc.dist <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(pc.dist, method= "ward.D2")
plot(wisc.pr.hclust)
```


Cut the deprogram/tree into two main groups/clusters:

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

I wan to know how the clustering in `grps` with values of 1 or 2 correspond to the expert `diagnosis`

```{r}
table(grps, diagnosis)
```

My clustering **groups 1** are mostly "M" diagnosis (179) and my clustering **group 2** are mostly "B" diagnosis 

24 FP
179 TP
333 TN
33 FN

Sensitivity TP/(TP+FN)
```{r}
179/(179+33)
```
Specificity TN/(TN+FP)
```{r}
333/(333+24)
```

> Q15. OPTIONAL: Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?
There is high specificity and strong sensitivity compared to clustering on the orignial data

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q16. Which of these new patients should we prioritize for follow up based on your results?
Cluster 1 should be prioritized for a follow up because they are malignant.
